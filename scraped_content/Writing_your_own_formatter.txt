¶
SubclassingHTMLFormatterorXMLFormatterwill
give you even more control over the output. For example, Beautiful
Soup sorts the attributes in every tag by default:
HTMLFormatter
HTMLFormatter
HTMLFormatter
XMLFormatter
XMLFormatter
XMLFormatter
attr_soup=BeautifulSoup(b'<p z="1" m="2" a="3"></p>','html.parser')print(attr_soup.p.encode())# <p a="3" m="2" z="1"></p>
attr_soup=BeautifulSoup(b'<p z="1" m="2" a="3"></p>','html.parser')print(attr_soup.p.encode())# <p a="3" m="2" z="1"></p>
attr_soup=BeautifulSoup(b'<p z="1" m="2" a="3"></p>','html.parser')print(attr_soup.p.encode())# <p a="3" m="2" z="1"></p>

attr_soup
=
BeautifulSoup
(
b
'<p z="1" m="2" a="3"></p>'
,
'html.parser'
)
print
(
attr_soup
.
p
.
encode
())
# <p a="3" m="2" z="1"></p>
To turn this off, you can subclass theFormatter.attributes()method, which controls which attributes are output and in what
order. This implementation also filters out the attribute called "m"
whenever it appears:
Formatter.attributes()
Formatter.attributes()
classUnsortedAttributes(HTMLFormatter):defattributes(self,tag):fork,vintag.attrs.items():ifk=='m':continueyieldk,vprint(attr_soup.p.encode(formatter=UnsortedAttributes()))# <p z="1" a="3"></p>
classUnsortedAttributes(HTMLFormatter):defattributes(self,tag):fork,vintag.attrs.items():ifk=='m':continueyieldk,vprint(attr_soup.p.encode(formatter=UnsortedAttributes()))# <p z="1" a="3"></p>
classUnsortedAttributes(HTMLFormatter):defattributes(self,tag):fork,vintag.attrs.items():ifk=='m':continueyieldk,vprint(attr_soup.p.encode(formatter=UnsortedAttributes()))# <p z="1" a="3"></p>

class
UnsortedAttributes
(
HTMLFormatter
):
def
attributes
(
self
,
tag
):
for
k
,
v
in
tag
.
attrs
.
items
():
if
k
==
'm'
:
continue
yield
k
,
v
print
(
attr_soup
.
p
.
encode
(
formatter
=
UnsortedAttributes
()))
# <p z="1" a="3"></p>
One last caveat: if you create aCDataobject, the text inside
that object is always presentedexactly as it appears, with no
formatting. Beautiful Soup will call your entity substitution
function, just in case you've written a custom function that counts
all the strings in the document or something, but it will ignore the
return value:
CData
CData
CData
exactly as it appears, with no
formatting
frombs4.elementimportCDatasoup=BeautifulSoup("<a></a>",'html.parser')soup.a.string=CData("one < three")print(soup.a.prettify(formatter="html"))# <a>#  <![CDATA[one < three]]># </a>
frombs4.elementimportCDatasoup=BeautifulSoup("<a></a>",'html.parser')soup.a.string=CData("one < three")print(soup.a.prettify(formatter="html"))# <a>#  <![CDATA[one < three]]># </a>
frombs4.elementimportCDatasoup=BeautifulSoup("<a></a>",'html.parser')soup.a.string=CData("one < three")print(soup.a.prettify(formatter="html"))# <a>#  <![CDATA[one < three]]># </a>

from
bs4.element
import
CData
soup
=
BeautifulSoup
(
"<a></a>"
,
'html.parser'
)
soup
.
a
.
string
=
CData
(
"one < three"
)
print
(
soup
.
a
.
prettify
(
formatter
=
"html"
))
# <a>
#  <![CDATA[one < three]]>
# </a>
get_text()¶If you only want the human-readable text inside a document or tag, you can use theget_text()method. It returns all the text in a document or
beneath a tag, as a single Unicode string:markup='<a href="http://example.com/">\nI linked to <i>example.com</i>\n</a>'soup=BeautifulSoup(markup,'html.parser')soup.get_text()'\nI linked to example.com\n'soup.i.get_text()'example.com'You can specify a string to be used to join the bits of text
together:# soup.get_text("|")'\nI linked to |example.com|\n'You can tell Beautiful Soup to strip whitespace from the beginning and
end of each bit of text:# soup.get_text("|", strip=True)'I linked to|example.com'But at that point you might want to use the.stripped_stringsgenerator instead, and process the text yourself:[textfortextinsoup.stripped_strings]# ['I linked to', 'example.com']As of Beautiful Soup version 4.9.0, when lxml or html.parser are in
use, the contents of <script>, <style>, and <template>
tags are generally not considered to be 'text', since those tags are not part of
the human-visible content of the page.As of Beautiful Soup version 4.10.0, you can call get_text(),
.strings, or .stripped_strings on a NavigableString object. It will
either return the object itself, or nothing, so the only reason to do
this is when you're iterating over a mixed list.As of Beautiful Soup version 4.13.0, you can call .string on a
NavigableString object. It will return the object itself, so again,
the only reason to do this is when you're iterating over a mixed
list.
get_text()¶
get_text()
get_text()
¶
If you only want the human-readable text inside a document or tag, you can use theget_text()method. It returns all the text in a document or
beneath a tag, as a single Unicode string:
get_text()
get_text()
markup='<a href="http://example.com/">\nI linked to <i>example.com</i>\n</a>'soup=BeautifulSoup(markup,'html.parser')soup.get_text()'\nI linked to example.com\n'soup.i.get_text()'example.com'
markup='<a href="http://example.com/">\nI linked to <i>example.com</i>\n</a>'soup=BeautifulSoup(markup,'html.parser')soup.get_text()'\nI linked to example.com\n'soup.i.get_text()'example.com'
markup='<a href="http://example.com/">\nI linked to <i>example.com</i>\n</a>'soup=BeautifulSoup(markup,'html.parser')soup.get_text()'\nI linked to example.com\n'soup.i.get_text()'example.com'

markup
=
'<a href="http://example.com/">
\n
I linked to <i>example.com</i>
\n
</a>'
soup
=
BeautifulSoup
(
markup
,
'html.parser'
)
soup
.
get_text
()
'
\n
I linked to example.com
\n
'
soup
.
i
.
get_text
()
'example.com'
You can specify a string to be used to join the bits of text
together:
# soup.get_text("|")'\nI linked to |example.com|\n'
# soup.get_text("|")'\nI linked to |example.com|\n'
# soup.get_text("|")'\nI linked to |example.com|\n'

# soup.get_text("|")
'
\n
I linked to |example.com|
\n
'
You can tell Beautiful Soup to strip whitespace from the beginning and
end of each bit of text:
# soup.get_text("|", strip=True)'I linked to|example.com'
# soup.get_text("|", strip=True)'I linked to|example.com'
# soup.get_text("|", strip=True)'I linked to|example.com'

# soup.get_text("|", strip=True)
'I linked to|example.com'
But at that point you might want to use the.stripped_stringsgenerator instead, and process the text yourself:
.stripped_strings
.stripped_strings
[textfortextinsoup.stripped_strings]# ['I linked to', 'example.com']
[textfortextinsoup.stripped_strings]# ['I linked to', 'example.com']
[textfortextinsoup.stripped_strings]# ['I linked to', 'example.com']

[
text
for
text
in
soup
.
stripped_strings
]
# ['I linked to', 'example.com']
As of Beautiful Soup version 4.9.0, when lxml or html.parser are in
use, the contents of <script>, <style>, and <template>
tags are generally not considered to be 'text', since those tags are not part of
the human-visible content of the page.
As of Beautiful Soup version 4.9.0, when lxml or html.parser are in
use, the contents of <script>, <style>, and <template>
tags are generally not considered to be 'text', since those tags are not part of
the human-visible content of the page.
As of Beautiful Soup version 4.10.0, you can call get_text(),
.strings, or .stripped_strings on a NavigableString object. It will
either return the object itself, or nothing, so the only reason to do
this is when you're iterating over a mixed list.
As of Beautiful Soup version 4.10.0, you can call get_text(),
.strings, or .stripped_strings on a NavigableString object. It will
either return the object itself, or nothing, so the only reason to do
this is when you're iterating over a mixed list.
As of Beautiful Soup version 4.13.0, you can call .string on a
NavigableString object. It will return the object itself, so again,
the only reason to do this is when you're iterating over a mixed
list.
As of Beautiful Soup version 4.13.0, you can call .string on a
NavigableString object. It will return the object itself, so again,
the only reason to do this is when you're iterating over a mixed
list.
Specifying the parser to use¶If you just need to parse some HTML, you can dump the markup into theBeautifulSoupconstructor, and it'll probably be fine. Beautiful
Soup will pick a parser for you and parse the data. But there are a
few additional arguments you can pass in to the constructor to change
which parser is used.The first argument to theBeautifulSoupconstructor is a string or
an open filehandle—the source of the markup you want parsed. The second
argument ishowyou'd like the markup parsed.If you don't specify anything, you'll get the best HTML parser that's
installed. Beautiful Soup ranks lxml's parser as being the best, then
html5lib's, then Python's built-in parser. You can override this by
specifying one of the following:What type of markup you want to parse. Currently supported values are
"html", "xml", and "html5".The name of the parser library you want to use. Currently supported
options are "lxml", "html5lib", and "html.parser" (Python's
built-in HTML parser).The sectionInstalling a parsercontrasts the supported parsers.If you ask for a parser that isn't installed, Beautiful Soup will
raise an exception so that you don't inadvertently parse a document
under an unknown set of rules. For example, right now, the only
supported XML parser is lxml. If you don't have lxml installed, asking
for an XML parser won't give you one, and asking for "lxml" won't work
either.Differences between parsers¶Beautiful Soup presents the same interface to a number of different
parsers, but each parser is different. Different parsers will create
different parse trees from the same document. The biggest differences
are between the HTML parsers and the XML parsers. Here's a short
document, parsed as HTML using the parser that comes with Python:BeautifulSoup("<a><b/></a>","html.parser")# <a><b></b></a>Since a standalone <b/> tag is not valid HTML, html.parser turns it into
a <b></b> tag pair.Here's the same document parsed as XML (running this requires that you
have lxml installed). Note that the standalone <b/> tag is left alone, and
that the document is given an XML declaration instead of being put
into an <html> tag.:print(BeautifulSoup("<a><b/></a>","xml"))# <?xml version="1.0" encoding="utf-8"?># <a><b/></a>There are also differences between HTML parsers. If you give Beautiful
Soup a perfectly-formed HTML document, these differences won't
matter. One parser will be faster than another, but they'll all give
you a data structure that looks exactly like the original HTML
document.But if the document is not perfectly-formed, different parsers will
give different results. Here's a short, invalid document parsed using
lxml's HTML parser. Note that the <a> tag gets wrapped in <body> and
<html> tags, and the dangling </p> tag is simply ignored:BeautifulSoup("<a></p>","lxml")# <html><body><a></a></body></html>Here's the same document parsed using html5lib:BeautifulSoup("<a></p>","html5lib")# <html><head></head><body><a><p></p></a></body></html>Instead of ignoring the dangling </p> tag, html5lib pairs it with an
opening <p> tag. html5lib also adds an empty <head> tag; lxml didn't
bother.Here's the same document parsed with Python's built-in HTML
parser:BeautifulSoup("<a></p>","html.parser")# <a></a>Like lxml, this parser ignores the closing </p> tag. Unlike
html5lib or lxml, this parser makes no attempt to create a
well-formed HTML document by adding <html> or <body> tags.Since the document "<a></p>" is invalid, none of these techniques is
the 'correct' way to handle it. The html5lib parser uses techniques
that are part of the HTML5 standard, so it has the best claim on being
the 'correct' way, but all three techniques are legitimate.Differences between parsers can affect your script. If you're planning
on distributing your script to other people, or running it on multiple
machines, you should specify a parser in theBeautifulSoupconstructor. That will reduce the chances that your users parse a
document differently from the way you parse it.