¶
If you just need to parse some HTML, you can dump the markup into theBeautifulSoupconstructor, and it'll probably be fine. Beautiful
Soup will pick a parser for you and parse the data. But there are a
few additional arguments you can pass in to the constructor to change
which parser is used.
BeautifulSoup
BeautifulSoup
The first argument to theBeautifulSoupconstructor is a string or
an open filehandle—the source of the markup you want parsed. The second
argument ishowyou'd like the markup parsed.
BeautifulSoup
BeautifulSoup
how
If you don't specify anything, you'll get the best HTML parser that's
installed. Beautiful Soup ranks lxml's parser as being the best, then
html5lib's, then Python's built-in parser. You can override this by
specifying one of the following:
What type of markup you want to parse. Currently supported values are
"html", "xml", and "html5".The name of the parser library you want to use. Currently supported
options are "lxml", "html5lib", and "html.parser" (Python's
built-in HTML parser).
What type of markup you want to parse. Currently supported values are
"html", "xml", and "html5".
What type of markup you want to parse. Currently supported values are
"html", "xml", and "html5".
The name of the parser library you want to use. Currently supported
options are "lxml", "html5lib", and "html.parser" (Python's
built-in HTML parser).
The name of the parser library you want to use. Currently supported
options are "lxml", "html5lib", and "html.parser" (Python's
built-in HTML parser).
The sectionInstalling a parsercontrasts the supported parsers.
Installing a parser
If you ask for a parser that isn't installed, Beautiful Soup will
raise an exception so that you don't inadvertently parse a document
under an unknown set of rules. For example, right now, the only
supported XML parser is lxml. If you don't have lxml installed, asking
for an XML parser won't give you one, and asking for "lxml" won't work
either.
Differences between parsers¶Beautiful Soup presents the same interface to a number of different
parsers, but each parser is different. Different parsers will create
different parse trees from the same document. The biggest differences
are between the HTML parsers and the XML parsers. Here's a short
document, parsed as HTML using the parser that comes with Python:BeautifulSoup("<a><b/></a>","html.parser")# <a><b></b></a>Since a standalone <b/> tag is not valid HTML, html.parser turns it into
a <b></b> tag pair.Here's the same document parsed as XML (running this requires that you
have lxml installed). Note that the standalone <b/> tag is left alone, and
that the document is given an XML declaration instead of being put
into an <html> tag.:print(BeautifulSoup("<a><b/></a>","xml"))# <?xml version="1.0" encoding="utf-8"?># <a><b/></a>There are also differences between HTML parsers. If you give Beautiful
Soup a perfectly-formed HTML document, these differences won't
matter. One parser will be faster than another, but they'll all give
you a data structure that looks exactly like the original HTML
document.But if the document is not perfectly-formed, different parsers will
give different results. Here's a short, invalid document parsed using
lxml's HTML parser. Note that the <a> tag gets wrapped in <body> and
<html> tags, and the dangling </p> tag is simply ignored:BeautifulSoup("<a></p>","lxml")# <html><body><a></a></body></html>Here's the same document parsed using html5lib:BeautifulSoup("<a></p>","html5lib")# <html><head></head><body><a><p></p></a></body></html>Instead of ignoring the dangling </p> tag, html5lib pairs it with an
opening <p> tag. html5lib also adds an empty <head> tag; lxml didn't
bother.Here's the same document parsed with Python's built-in HTML
parser:BeautifulSoup("<a></p>","html.parser")# <a></a>Like lxml, this parser ignores the closing </p> tag. Unlike
html5lib or lxml, this parser makes no attempt to create a
well-formed HTML document by adding <html> or <body> tags.Since the document "<a></p>" is invalid, none of these techniques is
the 'correct' way to handle it. The html5lib parser uses techniques
that are part of the HTML5 standard, so it has the best claim on being
the 'correct' way, but all three techniques are legitimate.Differences between parsers can affect your script. If you're planning
on distributing your script to other people, or running it on multiple
machines, you should specify a parser in theBeautifulSoupconstructor. That will reduce the chances that your users parse a
document differently from the way you parse it.
Differences between parsers¶
¶
Beautiful Soup presents the same interface to a number of different
parsers, but each parser is different. Different parsers will create
different parse trees from the same document. The biggest differences
are between the HTML parsers and the XML parsers. Here's a short
document, parsed as HTML using the parser that comes with Python:
BeautifulSoup("<a><b/></a>","html.parser")# <a><b></b></a>
BeautifulSoup("<a><b/></a>","html.parser")# <a><b></b></a>
BeautifulSoup("<a><b/></a>","html.parser")# <a><b></b></a>

BeautifulSoup
(
"<a><b/></a>"
,
"html.parser"
)
# <a><b></b></a>
Since a standalone <b/> tag is not valid HTML, html.parser turns it into
a <b></b> tag pair.
Here's the same document parsed as XML (running this requires that you
have lxml installed). Note that the standalone <b/> tag is left alone, and
that the document is given an XML declaration instead of being put
into an <html> tag.:
print(BeautifulSoup("<a><b/></a>","xml"))# <?xml version="1.0" encoding="utf-8"?># <a><b/></a>
print(BeautifulSoup("<a><b/></a>","xml"))# <?xml version="1.0" encoding="utf-8"?># <a><b/></a>
print(BeautifulSoup("<a><b/></a>","xml"))# <?xml version="1.0" encoding="utf-8"?># <a><b/></a>

print
(
BeautifulSoup
(
"<a><b/></a>"
,
"xml"
))
# <?xml version="1.0" encoding="utf-8"?>
# <a><b/></a>
There are also differences between HTML parsers. If you give Beautiful
Soup a perfectly-formed HTML document, these differences won't
matter. One parser will be faster than another, but they'll all give
you a data structure that looks exactly like the original HTML
document.
But if the document is not perfectly-formed, different parsers will
give different results. Here's a short, invalid document parsed using
lxml's HTML parser. Note that the <a> tag gets wrapped in <body> and
<html> tags, and the dangling </p> tag is simply ignored:
BeautifulSoup("<a></p>","lxml")# <html><body><a></a></body></html>
BeautifulSoup("<a></p>","lxml")# <html><body><a></a></body></html>
BeautifulSoup("<a></p>","lxml")# <html><body><a></a></body></html>

BeautifulSoup
(
"<a></p>"
,
"lxml"
)
# <html><body><a></a></body></html>
Here's the same document parsed using html5lib:
BeautifulSoup("<a></p>","html5lib")# <html><head></head><body><a><p></p></a></body></html>
BeautifulSoup("<a></p>","html5lib")# <html><head></head><body><a><p></p></a></body></html>
BeautifulSoup("<a></p>","html5lib")# <html><head></head><body><a><p></p></a></body></html>

BeautifulSoup
(
"<a></p>"
,
"html5lib"
)
# <html><head></head><body><a><p></p></a></body></html>
Instead of ignoring the dangling </p> tag, html5lib pairs it with an
opening <p> tag. html5lib also adds an empty <head> tag; lxml didn't
bother.
Here's the same document parsed with Python's built-in HTML
parser:
BeautifulSoup("<a></p>","html.parser")# <a></a>
BeautifulSoup("<a></p>","html.parser")# <a></a>
BeautifulSoup("<a></p>","html.parser")# <a></a>

BeautifulSoup
(
"<a></p>"
,
"html.parser"
)
# <a></a>
Like lxml, this parser ignores the closing </p> tag. Unlike
html5lib or lxml, this parser makes no attempt to create a
well-formed HTML document by adding <html> or <body> tags.
Since the document "<a></p>" is invalid, none of these techniques is
the 'correct' way to handle it. The html5lib parser uses techniques
that are part of the HTML5 standard, so it has the best claim on being
the 'correct' way, but all three techniques are legitimate.
Differences between parsers can affect your script. If you're planning
on distributing your script to other people, or running it on multiple
machines, you should specify a parser in theBeautifulSoupconstructor. That will reduce the chances that your users parse a
document differently from the way you parse it.
BeautifulSoup
BeautifulSoup
Encodings¶Any HTML or XML document is written in a specific encoding like ASCII
or UTF-8. But when you load that document into Beautiful Soup, you'll
discover it's been converted to Unicode:markup=b"<h1>Sacr\xc3\xa9bleu!</h1>"soup=BeautifulSoup(markup,'html.parser')soup.h1# <h1>Sacré bleu!</h1>soup.h1.string# 'Sacr\xe9 bleu!'It's not magic. (That sure would be nice.) Beautiful Soup uses a
sub-library calledUnicode, Dammitto detect a document's encoding
and convert it to Unicode. The autodetected encoding is available as
the.original_encodingattribute of theBeautifulSoupobject:soup.original_encoding# 'utf-8'If.original_encodingisNone, that means the document was
already Unicode when it was passed into Beautiful Soup:markup="<h1>Sacré bleu!</h1>"soup=BeautifulSoup(markup,'html.parser')print(soup.original_encoding)# NoneUnicode, Dammit guesses correctly most of the time, but sometimes it
makes mistakes. Sometimes it guesses correctly, but only after a
byte-by-byte search of the document that takes a very long time. If
you happen to know a document's encoding ahead of time, you can avoid
mistakes and delays by passing it to theBeautifulSoupconstructor
asfrom_encoding.Here's a document written in ISO-8859-8. The document is so short that
Unicode, Dammit can't get a lock on it, and misidentifies it as
ISO-8859-7:markup=b"<h1>\xed\xe5\xec\xf9</h1>"soup=BeautifulSoup(markup,'html.parser')print(soup.h1)# <h1>νεμω</h1>print(soup.original_encoding)# iso-8859-7We can fix this by passing in the correctfrom_encoding:soup=BeautifulSoup(markup,'html.parser',from_encoding="iso-8859-8")print(soup.h1)# <h1>םולש</h1>print(soup.original_encoding)# iso8859-8If you don't know what the correct encoding is, but you know that
Unicode, Dammit is guessing wrong, you can pass the wrong guesses in
asexclude_encodings:soup=BeautifulSoup(markup,'html.parser',exclude_encodings=["iso-8859-7"])print(soup.h1)# <h1>םולש</h1>print(soup.original_encoding)# WINDOWS-1255Windows-1255 isn't 100% correct, but that encoding is a compatible
superset of ISO-8859-8, so it's close enough. (exclude_encodingsis a new feature in Beautiful Soup 4.4.0.)In rare cases (usually when a UTF-8 document contains text written in
a completely different encoding), the only way to get Unicode may be
to replace some characters with the special Unicode character
"REPLACEMENT CHARACTER" (U+FFFD, �). If Unicode, Dammit needs to do
this, it will set the.contains_replacement_charactersattribute
toTrueon theUnicodeDammitorBeautifulSoupobject. This
lets you know that the Unicode representation is not an exact
representation of the original—some data was lost. If a document
contains �, but.contains_replacement_charactersisFalse,
you'll know that the � was there originally (as it is in this
paragraph) and doesn't stand in for missing data.Output encoding¶When you write out an output document from Beautiful Soup, you get a UTF-8
document, even if the input document wasn't in UTF-8 to begin with. Here's a
document written in the Latin-1 encoding:markup=b'''<html><head><meta content="text/html; charset=ISO-Latin-1" http-equiv="Content-type" /></head><body><p>Sacr\xe9bleu!</p></body></html>'''soup=BeautifulSoup(markup,'html.parser')print(soup.prettify())# <html>#  <head>#   <meta content="text/html; charset=utf-8" http-equiv="Content-type" />#  </head>#  <body>#   <p>#    Sacré bleu!#   </p>#  </body># </html>Note that the <meta> tag has been rewritten to reflect the fact that
the document is now in UTF-8.If you don't want UTF-8, you can pass an encoding intoprettify():print(soup.prettify("latin-1"))# <html>#  <head>#   <meta content="text/html; charset=latin-1" http-equiv="Content-type" /># ...You can also call encode() on theBeautifulSoupobject, or any
element in the soup, just as if it were a Python string:soup.p.encode("latin-1")# b'<p>Sacr\xe9 bleu!</p>'soup.p.encode("utf-8")# b'<p>Sacr\xc3\xa9 bleu!</p>'Any characters that can't be represented in your chosen encoding will
be converted into numeric XML entity references. Here's a document
that includes the Unicode character SNOWMAN:markup=u"<b>\N{SNOWMAN}</b>"snowman_soup=BeautifulSoup(markup,'html.parser')tag=snowman_soup.bThe SNOWMAN character can be part of a UTF-8 document (it looks like
☃), but there's no representation for that character in ISO-Latin-1 or
ASCII, so it's converted into "&#9731" for those encodings:print(tag.encode("utf-8"))# b'<b>\xe2\x98\x83</b>'print(tag.encode("latin-1"))# b'<b>&#9731;</b>'print(tag.encode("ascii"))# b'<b>&#9731;</b>'Unicode, Dammit¶You can use Unicode, Dammit without using Beautiful Soup. It's useful
whenever you have data in an unknown encoding and you just want it to
become Unicode:frombs4importUnicodeDammitdammit=UnicodeDammit(b"\xc2\xabSacr\xc3\xa9bleu!\xc2\xbb")print(dammit.unicode_markup)# «Sacré bleu!»dammit.original_encoding# 'utf-8'Unicode, Dammit's guesses will get a lot more accurate if you install
one of these Python libraries:charset-normalizer,chardet, orcchardet. The more data you give Unicode, Dammit, the more
accurately it will guess. If you have your own suspicions as to what
the encoding might be, you can pass them in as a list:dammit=UnicodeDammit("Sacr\xe9bleu!",["latin-1","iso-8859-1"])print(dammit.unicode_markup)# Sacré bleu!dammit.original_encoding# 'latin-1'Unicode, Dammit has two special features that Beautiful Soup doesn't
use.Smart quotes¶You can use Unicode, Dammit to convert Microsoft smart quotes to HTML or XML
entities:markup=b"<p>I just\x93love\x94Microsoft Word\x92s smart quotes</p>"UnicodeDammit(markup,["windows-1252"],smart_quotes_to="html").unicode_markup# '<p>I just &ldquo;love&rdquo; Microsoft Word&rsquo;s smart quotes</p>'UnicodeDammit(markup,["windows-1252"],smart_quotes_to="xml").unicode_markup# '<p>I just &#x201C;love&#x201D; Microsoft Word&#x2019;s smart quotes</p>'You can also convert Microsoft smart quotes to ASCII quotes:UnicodeDammit(markup,["windows-1252"],smart_quotes_to="ascii").unicode_markup# '<p>I just "love" Microsoft Word\'s smart quotes</p>'Hopefully you'll find this feature useful, but Beautiful Soup doesn't
use it. Beautiful Soup prefers the default behavior, which is to
convert Microsoft smart quotes to Unicode characters along with
everything else:UnicodeDammit(markup,["windows-1252"]).unicode_markup# '<p>I just “love” Microsoft Word’s smart quotes</p>'Inconsistent encodings¶Sometimes a document is mostly in UTF-8, but contains Windows-1252
characters such as (again) Microsoft smart quotes. This can happen
when a website includes data from multiple sources. You can useUnicodeDammit.detwingle()to turn such a document into pure
UTF-8. Here's a simple example:snowmen=(u"\N{SNOWMAN}"*3)quote=(u"\N{LEFT DOUBLE QUOTATION MARK}I like snowmen!\N{RIGHT DOUBLE QUOTATION MARK}")doc=snowmen.encode("utf8")+quote.encode("windows_1252")This document is a mess. The snowmen are in UTF-8 and the quotes are
in Windows-1252. You can display the snowmen or the quotes, but not
both:print(doc)# ☃☃☃�I like snowmen!�print(doc.decode("windows-1252"))# â˜ƒâ˜ƒâ˜ƒ“I like snowmen!”Decoding the document as UTF-8 raises aUnicodeDecodeError, and
decoding it as Windows-1252 gives you gibberish. Fortunately,UnicodeDammit.detwingle()will convert the string to pure UTF-8,
allowing you to decode it to Unicode and display the snowmen and quote
marks simultaneously:new_doc=UnicodeDammit.detwingle(doc)print(new_doc.decode("utf8"))# ☃☃☃“I like snowmen!”UnicodeDammit.detwingle()only knows how to handle Windows-1252
embedded in UTF-8 (or vice versa, I suppose), but this is the most
common case.Note that you must know to callUnicodeDammit.detwingle()on your
data before passing it intoBeautifulSoupor theUnicodeDammitconstructor. Beautiful Soup assumes that a document has a single
encoding, whatever it might be. If you pass it a document that
contains both UTF-8 and Windows-1252, it's likely to think the whole
document is Windows-1252, and the document will come out looking likeâ˜ƒâ˜ƒâ˜ƒ“Ilikesnowmen!”.UnicodeDammit.detwingle()is new in Beautiful Soup 4.1.0.